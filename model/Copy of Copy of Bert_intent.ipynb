{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[{"file_id":"1aBy4A7qH2kLhXExg1bzU6v5cONa_6Ilm","timestamp":1664327171449},{"file_id":"1_BRvwMad2Y5iDAZSNqLoSD4HZDt5pS9Q","timestamp":1664147512925}],"mount_file_id":"1_BRvwMad2Y5iDAZSNqLoSD4HZDt5pS9Q","authorship_tag":"ABX9TyM3aCE4UsoAvgBfqonu/H4q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XEJRyCVA6eKi"},"outputs":[],"source":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"UoW47SjA6hKF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A GPU can be added by going to the menu and selecting: Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)\n","# confirm the GPU is detected:\n","\n","import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\n","    "],"metadata":{"id":"dofsXoHI6h6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device\n","\n","import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"tjVzXIySBuEK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers "],"metadata":{"id":"cjdOqZ90U_Jt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install transformers\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import json"],"metadata":{"id":"sbqmKqwWB8CW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pwd"],"metadata":{"id":"IDjBPtug-vx8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data = pd.read_csv('drive/MyDrive/Bert_intent/overview-of-recordings.csv')\n","f = open('drive/MyDrive/data_oos_plus.json','r')\n","data = json.load(f)\n","\n","df = pd.DataFrame(data['train'] + data['oos_train'], columns = ['phrase', 'prompt'])\n","df_test = pd.DataFrame(data['test'] + data['oos_test'], columns = ['phrase', 'prompt'])\n","# len(arr)\n","df.head()\n","# df.isna().any()\n","print(len(df), len(df_test))\n","# df_test = pd.DataFrame()\n","df.to_csv('./train.csv')\n","df_test.to_csv('./test.csv')"],"metadata":{"id":"aQ9hEpMKCBXP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df=data1.copy()\n","# df.isna().sum()\n","df_test.head()"],"metadata":{"id":"nWGmBF67Uhwn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['prompt'].value_counts()"],"metadata":{"id":"YtSM2R5AUr8s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Total number of intents: %d'%(len(df['prompt'].value_counts().index)))"],"metadata":{"id":"iuiQgs8MUu8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X, sentence_test, y, intent_test = train_test_split(df.phrase, df.prompt, stratify = df.prompt,test_size=0.2, random_state=4612)\n","sentence_train, sentence_val, intent_train, intent_val = train_test_split(X, y, stratify = y,test_size=0.125, random_state=4612)\n","\n"],"metadata":{"id":"cNiEpuOKU0ul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"#examples in training set:{ sentence_train.shape[0]}\\n#examples in validation set:{ sentence_val.shape[0]}\\n#examples in test set:{ sentence_test.shape[0]}\")"],"metadata":{"id":"b8LDLX1eU20h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining some key variables that will be used later on in the training\n","TRAIN_BATCH_SIZE =32\n","VALID_BATCH_SIZE = 64\n","EPSILON = 1e-05\n","EPOCHS = 10\n","LEARNING_RATE = 1e-5\n","SEED = 1215\n","from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"yGBJES5sU48W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 0\n","input = []\n","length=[]\n","# For every sentence...\n","for sent in sentence_train:\n","\n","    # Tokenize the text and add special tokens--`[CLS]` and `[SEP]` tokens.\n","    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","    input.append(input_ids)\n","    length.append(len(input_ids))\n","    # Update the maximum sentence length.\n","    max_len = max(max_len, len(input_ids))\n","    mean_len = sum(length)/len(length)\n","#39 tokens is the maximum number of tokens in a sentence (transcription). Also, a sentence has 14 tokens on average.\n","print('Max sentence length:%d \\nMean sentence length:%d' % (max_len,mean_len))"],"metadata":{"id":"tXADa9utU7kU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create a function to tokenize sentences.  \n","def tokenize(sentence):\n","  batch = tokenizer(list(sentence),             \n","                  is_pretokenized=False,\n","                  #Pad or truncate all sentences to the same length. Create the attention masks which explicitly differentiate real tokens from [PAD] tokens.\n","                  padding=True, \n","                  truncation=True,\n","                  return_tensors=\"pt\")\n","  return batch"],"metadata":{"id":"FfvqekJtVIbX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok_train = tokenize(sentence_train)\n","tok_val = tokenize(sentence_val)\n","tok_test = tokenize(sentence_test)\n","\n"],"metadata":{"id":"RaBmm7q_VL9y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tok_train"],"metadata":{"id":"0Jh_ZGyRVN6M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import LabelEncoder\n","# encode \"intent\" to 25 number labels\n","LE = LabelEncoder()\n","label_train = torch.tensor((LE.fit_transform(intent_train)))\n","label_val = torch.tensor((LE.fit_transform(intent_val)))\n","label_test = torch.tensor((LE.fit_transform(intent_test)))\n","\n","print(label_train)\n","\n"],"metadata":{"id":"R499VRplVhEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import TensorDataset\n","\n","train_dataset = TensorDataset(tok_train['input_ids'], tok_train['attention_mask'],label_train)\n","validation_dataset = TensorDataset(tok_val['input_ids'], tok_val['attention_mask'],label_val)\n","test_dataset = TensorDataset(tok_test['input_ids'], tok_test['attention_mask'],label_test)\n","\n"],"metadata":{"id":"QJL4X-KPVj2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset"],"metadata":{"id":"uJKsp24jVnrO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# Create the DataLoaders for our training and validation sets.\n","# We'll take training samples in random order. \n","train_dataloader = DataLoader(\n","            train_dataset,  # The training samples.\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = TRAIN_BATCH_SIZE # Trains with this batch size.\n","        )\n","\n","# For validation/test the order doesn't matter, so we'll just read them sequentially.\n","validation_dataloader = DataLoader(\n","            validation_dataset, # The validation samples.\n","            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n","            batch_size = VALID_BATCH_SIZE # Evaluate with this batch size.\n","        )\n","\n","test_dataloader = DataLoader(\n","            validation_dataset, \n","            sampler = SequentialSampler(validation_dataset), \n","            batch_size = VALID_BATCH_SIZE \n","        )"],"metadata":{"id":"j4Qc7I4UVqcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !pip install pytorch_pretrained_bert==0.4.0\n","from transformers.optimization import get_scheduler"],"metadata":{"id":"K_Wic1ntXT8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","## use pretained base(relatively small) BERT mdoel for sequence classification \n","#CUDA_LAUNCH_BLOCKING=1\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 151)\n","model.cuda() # make pytorch run this model on GPU.\n","\n","## use AdamW optimizer\n","optimizer = AdamW(model.parameters(), \n","                  lr = LEARNING_RATE, \n","                  eps = EPSILON) #very small number to prevent any division by zero )\n","\n","# from transformers import get_linear_schedule_with_warmup\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","total_steps = len(train_dataloader) * EPOCHS\n","\n","## Create the learning rate scheduler.\n","scheduler = get_scheduler(\"linear\", optimizer, \n","                          num_warmup_steps = 0, # Default value in run_glue.py\n","                          num_training_steps = total_steps)"],"metadata":{"id":"LAuJFvaIVswb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to calcuate the accuracy of the model\n","\n","def calcuate_accu(big_idx, targets):\n","    n_correct = (big_idx==targets).sum().item()\n","    return n_correct"],"metadata":{"id":"zrM04lViWeOb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    #Takes a time in seconds and returns a string hh:mm:ss\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))   \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"dspTxfDxdkHL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.tensorboard import SummaryWriter\n","\n","# default `log_dir` is \"runs\" - we'll be more specific here\n","writer = SummaryWriter('runs/Tensorboard')"],"metadata":{"id":"EW72JTgfdncX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start the training process:\n","import random\n","import torch\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","def train(epochs):\n","  total_t0 = time.time() # Measure the total training time for the whole run.\n","  tr_loss = 0\n","  n_correct = 0\n","  nb_tr_steps = 0\n","  nb_tr_examples = 0\n","  \n","  # For each epoch...\n","  for epoch in range(0, epochs):\n","      print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n","      print('Training...')\n","\n","      t0 = time.time()     # Measure how long the training epoch takes.\n","      total_tr_loss = 0\n","      total_n_correct = 0\n","      total_nb_tr_examples = 0\n","      model.train()    # Put the model into training mode\n","\n","      # For each batch of training data...\n","      for step, batch in enumerate(train_dataloader, 0):     \n","          # 'batch' contains three pytorch tensors:[0]: input ids, [1]: attention masks, [2]: labels \n","          input_ids = batch[0].to(device, dtype = torch.long)\n","          input_mask = batch[1].to(device, dtype = torch.long)\n","          labels = batch[2].to(device, dtype = torch.long)\n","\n","          model.zero_grad()       #clear any previously calculated gradients \n","\n","          outputs = model(input_ids, token_type_ids=None, attention_mask=input_mask)\n","          loss_function = torch.nn.CrossEntropyLoss()\n","          loss = loss_function(outputs[0], labels) #`loss` is a Tensor containing a single value\n","          tr_loss += loss.item() #.item()` function just returns the Python value from the tensor\n","          total_tr_loss += loss.item()\n","          big_val, big_idx = torch.max(outputs[0], dim=1)\n","          n_correct += calcuate_accu(big_idx, labels)  \n","          total_n_correct += calcuate_accu(big_idx, labels)                  \n","          nb_tr_steps += 1\n","          nb_tr_examples+=labels.size(0)\n","          total_nb_tr_examples+=labels.size(0)\n","\n","          if step % 20==19:\n","              loss_step = tr_loss/nb_tr_steps\n","              accu_step = n_correct/nb_tr_examples # #correct examples/all examples \n","              print(f\"Training Loss per 20 steps(batches): {loss_step}\")\n","              print(f\"Training Accuracy per 20 steps(batches): {accu_step}\")\n","              elapsed = format_time(time.time() - t0)    # Calculate elapsed time in minutes.   \n","              # Report progress.\n","              print('Batch {} of {}.  Elapsed: {:}.'.format(step+1, len(train_dataloader), elapsed))\n","              #writer.add_scalar('training loss', loss_step, (epoch +1)*len(trainloader) )\n","              tr_loss = 0;n_correct = 0;nb_tr_steps = 0;nb_tr_examples = 0\n","                \n","          loss.backward() # Perform a backward pass to calculate the gradients.\n","          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n","          optimizer.step()\n","          scheduler.step() # Update the learning rate.\n","\n","    # Calculate the average loss over all of the batches.\n","      train_loss_per_epoch = total_tr_loss / len(train_dataloader)            \n","      train_accuracy_per_epoch=total_n_correct/total_nb_tr_examples\n","      # Measure how long this epoch took.\n","      training_time = format_time(time.time() - t0)\n","\n","      print(\"\")\n","      print(\"training loss per epoch: {0:.2f}\".format(train_loss_per_epoch))\n","      print(\"training accuracy per epoch: {0:.2f}\".format(train_accuracy_per_epoch))\n","      print(\"Training 1 epcoh took: {:}\".format(training_time))"],"metadata":{"id":"2dFj2bbOdqlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train(epochs = EPOCHS)"],"metadata":{"id":"JW-YtXWKdveQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test the model on the validation set\n","def valid(model, validation_loader):\n","  model.eval()\n","  val_loss = 0\n","  nb_val_examples = 0\n","  n_correct = 0\n","  with torch.no_grad():\n","    for _, data in enumerate(validation_loader, 0): \n","      ids = data[0].to(device, dtype = torch.long)\n","      mask = data[1].to(device, dtype = torch.long)\n","      targets = data[2].to(device, dtype = torch.long)\n","      outputs = model(ids, mask)\n","      loss_function = torch.nn.CrossEntropyLoss()\n","      loss = loss_function(outputs[0], targets)\n","      val_loss += loss.item()\n","      big_val, big_idx = torch.max(outputs[0], dim=1)\n","      n_correct += calcuate_accu(big_idx, targets)\n","      nb_val_examples+=targets.size(0)\n","\n","  val_ave_loss = val_loss/len(validation_loader)\n","  val_accu = (n_correct*100)/nb_val_examples\n","  print(\"Loss on validation/test data: %0.2f\" % val_ave_loss)\n","  print(\"Accuracy on validation/test data: %0.2f%%\" % val_accu)\n","  \n","  return"],"metadata":{"id":"C-sabQKXd0Oq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid(model, validation_dataloader)"],"metadata":{"id":"BMkqb61qeX1k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid(model, test_dataloader)"],"metadata":{"id":"0hUp-BhmebUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './content/drive/MyDrive/Bert_intent/saved_bert_model_and_tokenizer_v3_final/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","  os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","\n"],"metadata":{"id":"Xb1PSFsleelu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_label = pd.DataFrame(tuple(zip(range(151),LE.classes_)), columns=['id','intent'])\n","df_label.to_pickle('./content/drive/MyDrive/Bert_intent/saved_bert_model_and_tokenizer_v3_final/df_label.pkl')"],"metadata":{"id":"FRwGjuAUe0_u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### load the model and build the detector for deployment\n","# !pip install transformers\n","import pandas as pd\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","input_dir = './content/drive/MyDrive/Bert_intent/saved_bert_model_and_tokenizer_v3_final/'\n","\n","loaded_model = BertForSequenceClassification.from_pretrained(input_dir)\n","loaded_model.eval()\n","loaded_tokenizer = BertTokenizer.from_pretrained(input_dir)\n","loaded_df_label = pd.read_pickle('./content/drive/MyDrive/Bert_intent/saved_bert_model_and_tokenizer_v3_final/df_label.pkl')\n","\n","\n"],"metadata":{"id":"A_S-A0PKfB0E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# test the model on an unseen example\n","\n","def medical_symptom_detector(intent):\n","\n","  pt_batch = loaded_tokenizer(\n","  intent,\n","  padding=True,\n","  truncation=True,\n","  return_tensors=\"pt\")\n","\n","  pt_outputs = loaded_model(**pt_batch)\n","  __, id = torch.max(pt_outputs[0], dim=1)\n","  prediction = loaded_df_label.iloc[[id.item()]]['intent'].item()\n","  print(prediction)\n","  # print('You may have a medical condition: %s. Would you like me to transfer your call to your doctor?'%(prediction))\n","  return "],"metadata":{"id":"R8WzE3a9fiqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["arr = data['test']\n","arr"],"metadata":{"id":"mnbo2dqLL5bW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input = \"How far is tony starks office ?\"\n","medical_symptom_detector(input)"],"metadata":{"id":"CWGKhZevgeYR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5RTvn74eghY2"},"execution_count":null,"outputs":[]}]}